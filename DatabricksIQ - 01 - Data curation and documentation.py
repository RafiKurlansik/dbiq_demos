# Databricks notebook source
# MAGIC %md
# MAGIC
# MAGIC # Accelerating analytics with DatabricksIQ
# MAGIC
# MAGIC DatabricksIQ is the engine that powers Databricks' Data Intelligence Platform. In addition to backend features like liquid clustering and serverless autoscaling which use machine learning to optimize performance, there are a number of features that make it easier for data professionals to discover, curate, and analyze data using generative AI.  This demo focuses on the various ways that DatabricksIQ can be used:
# MAGIC
# MAGIC * **Intelligent Search** to find the right data for your use case
# MAGIC * **AI-assisted Authoring** with the Assistant in the Notebook and SQL Editor
# MAGIC * **AI-generated documentation** for tables and columns in Unity Catalog
# MAGIC * **Genie** for talking to your data in natural language

# COMMAND ----------

# MAGIC %md
# MAGIC #### About this notebook
# MAGIC
# MAGIC * 99% of code is generated by the Assistant
# MAGIC * Prompts are included and can be re-executed to generate similar results  
# MAGIC * LLMs are probabilistic by nature, so the same prompt may generate different responses

# COMMAND ----------

# MAGIC %md
# MAGIC #### The data: IMDb top 1000
# MAGIC
# MAGIC What kinds of patterns might we find in the top 1000 movies of all time from IMDb?  Are critic favorites high grossing?  Which directors dominate the top rated films?  Which decade had the greatest number of movies in the top 1000?  
# MAGIC
# MAGIC To answer these basic questions we'll need to work through the typical stages of an analysis: ingesting, cleaning, visualizing and understanding the data.  In our case we will also make sure to _document_ the data that we are working with.  This will ensure that other teams will be able to leverage the work we've done without wondering where the data came from, or what the columns mean.

# COMMAND ----------

# MAGIC %md
# MAGIC ## Transforming data from unstructured to structured
# MAGIC
# MAGIC A standard task in data engineering is to extract structured data from unstructured data.  In any given cell of a table, there should be a single observation with a proper data type.  The IMDb dataset has several columns that are great examples of structured data being buried in an unstructured column.  **Normally this would require taking time to figure out the correct regular expression to parse the string, but the Databricks Assistant makes this task simple.**

# COMMAND ----------

# Read in the IMDb CSV from a Unity Catalog volume
imdb_raw = spark.read.csv("/Volumes/main/rkurlansik/data_science/IMDB_top_1000.csv", header=True, inferSchema=True).drop("_c0")

display(imdb_raw)

# COMMAND ----------

# MAGIC %md
# MAGIC Looking at the `Title` column, we can see that the release date is embedded in each cell.  This information belongs in its own column.  Let's clean this up by extracting the release date and removing any irrelevant integers (e.g., 1., 2., 3.).  To give the Assistant clear instructions, we'll use the cell highlight feature to copy an example into our prompt. If you aren't comfortable adding a real observation in your data to the prompt, consider making up an observation that has the same structure. That will help the Assistant generate quality results for you.<br><br>
# MAGIC
# MAGIC <img src="https://github.com/RafiKurlansik/notebook2/blob/main/assets/cell_highlight.gif?raw=true" width=700>
# MAGIC
# MAGIC Prompt: 
# MAGIC
# MAGIC _Here is an example of the Title column in our dataset: 1. The Shawshank Redemption (1994). The title name will be between the number and the parentheses, and the release date is between parentheses.  Write a function that extracts both the release date and the title name from the Title column in the imdb_raw DataFrame._

# COMMAND ----------

# DBTITLE 1,Extract Release Date and Title
from pyspark.sql.functions import regexp_extract

def extract_release_date_and_title(df):
  df = df.withColumn("ReleaseDate", regexp_extract(df.Title, r"\((\d{4})\)", 1))
  df = df.withColumn("Title", regexp_extract(df.Title, r"\d+\. (.*?) \(", 1))
  return df
  
imdb = extract_release_date_and_title(imdb_raw)
display(imdb)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Additional transformations
# MAGIC The `Info` column contains data on votes and gross revenue.  Let's use the Assistant to help us come up with a function to extract the revenue into its own column.  Use the cell highlight capability again and copy an example to share with the Assitant.
# MAGIC
# MAGIC Prompt:
# MAGIC
# MAGIC _In the imdb_raw DataFrame the Info column contains data that looks like this: Votes: 1,820,268 | Gross: $37.03M. Write a function to extract both the Gross revenue and the Votes from the Info column into their own columns._

# COMMAND ----------

from pyspark.sql.functions import regexp_extract

def extract_gross_revenue_and_votes(df):
    df = df.withColumn("Gross", regexp_extract(df.Info, "\\$[\\d,]+[.]?\\d*[MK]?", 0))
    df = df.withColumn("Votes", regexp_extract(df.Info, "^Votes: ([\\d,]+)", 1))
    return df

imdb = extract_gross_revenue_and_votes(imdb)
display(imdb)

# COMMAND ----------

# MAGIC %md
# MAGIC For our final example of creating structured data from unstructured data, let's extract the Director from the `Cast` column into its own column.
# MAGIC
# MAGIC Prompt:
# MAGIC _Please extract the Director from the Cast column in the imdb DataFrame using a regular expression.  Here's an example of the data: Director: Frank Darabont | Stars: Tim Robbins, Morgan Freeman, Bob Gunton, William Sadler_

# COMMAND ----------

from pyspark.sql.functions import regexp_extract

def extract_director(df):
    df = df.withColumn("Director", regexp_extract(df.Cast, "^Director: (.+?) \|", 1))
    return df

imdb = extract_director(imdb)
display(imdb)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Changing data types
# MAGIC The `Votes`, `Gross`, `ReleaseDate` and `Duration` columns are currently strings.  To enable an analysis of this data let's change them to the proper data types using the Assistant.
# MAGIC
# MAGIC Prompt:
# MAGIC _Write a function to first filter any rows with empty strings in Gross.  Then strip any non-integer characters from the `Duration`, `Votes` and `Gross` columns in the imdb DataFrame, then convert the resulting columns to integer type. Also convert `ReleaseDate` to year._

# COMMAND ----------

# DBTITLE 1,Clean and Convert IMDb Data
from pyspark.sql.functions import regexp_replace, col, year, to_date

def clean_and_convert(df):
    # Filter rows with empty strings in Gross column
    df = df.filter(col("Gross") != "")
    
    # Strip non-integer characters from Duration column and convert to integer type
    df = df.withColumn("Duration", regexp_replace(col("Duration"), "[^0-9]", "").cast("integer"))
    
    # Strip non-integer characters from Votes column and convert to integer type
    df = df.withColumn("Votes", regexp_replace(col("Votes"), "[^0-9]", "").cast("integer"))
    
    # Strip non-integer characters from Gross column and convert to integer type
    df = df.withColumn("Gross", regexp_replace(col("Gross"), "[^0-9]", "").cast("integer"))
    
    # Convert ReleaseDate to date with YYYY format
    df = df.withColumn("ReleaseDate", year(col("ReleaseDate")))
    
    return df

imdb = clean_and_convert(imdb)

# COMMAND ----------

# DBTITLE 1,Display IMDB2
display(imdb)

# COMMAND ----------

# MAGIC %md
# MAGIC We can also use the Assistant from the notebook cells directly:
# MAGIC
# MAGIC <img src='https://github.com/RafiKurlansik/notebook2/blob/main/assets/inline_generate_ai.gif?raw=true'>
# MAGIC
# MAGIC
# MAGIC
# MAGIC Prompt:
# MAGIC _Write a function to count the number of stars in the `Cast` column.  Make sure you skip the Directors, only counting everything after "Stars:"   Here's an example of the data: Director: Frank Darabont | Stars: Tim Robbins, Morgan Freeman, Bob Gunton, William Sadler_

# COMMAND ----------

# DBTITLE 1,Stars count in IMDB
from pyspark.sql.functions import split, size, col, regexp_extract

def count_stars(df):
  df = df.withColumn("Stars", regexp_extract(col("Cast"), "(?i)Stars: (.*)", 1))
  df = df.withColumn("Stars_Count", size(split(col("Stars"), ", ")))
  return df
  
imdb = count_stars(imdb)
display(imdb)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Diagnosing Errors
# MAGIC
# MAGIC If you run into errors during the course of your work, the Assistant will do its best to help out.  Let's create an error and see what happens. <br><br>
# MAGIC
# MAGIC <img src="https://github.com/RafiKurlansik/notebook2/blob/main/assets/diagnose_error.gif?raw=true" width=900>
# MAGIC
# MAGIC Try it yourself below:

# COMMAND ----------

# DBTITLE 1,Numeric String Concatenation
1 + "x"

# COMMAND ----------

# MAGIC %md
# MAGIC ## Save table to Unity Catalog
# MAGIC
# MAGIC When saving a table with the Assistant, it is best to be explicit about which schema you want to write to and any options you want to be used.  In this case we will stick with the default options.
# MAGIC
# MAGIC Prompt:
# MAGIC
# MAGIC _Save the imdb DataFrame as a delta lake table called "movies" to the main.imdb schema, using default options.  First drop the Info and Cast columns, then order the columns as follows: Title, ReleaseDate, Gross, Duration, Director, Genre, Description, Votes, Stars, Metascore._

# COMMAND ----------

# DBTITLE 1,Spark SQL IMDb Table Creation
# drop unnecessary columns
imdb = imdb.drop("Info", "Cast")

# select and reorder columns 
imdb = imdb.select("Title", "ReleaseDate", "Gross", "Duration", "Director", "Genre", "Description", "Votes", "Stars", "Metascore")

# save as a table in delta format
imdb.write.format("delta").mode("overwrite").saveAsTable("main.imdb.movies")

# COMMAND ----------

# MAGIC %md
# MAGIC ## AI-generated documentation for tables and columns in Unity Catalog
# MAGIC
# MAGIC Writing documentation about tables and columns you've created can be tedious.  Using AI to generate descriptions makes this task simple, leaving it to the developer to validate and augment the models' output with their own domain expertise.  

# COMMAND ----------

# MAGIC %md
# MAGIC ### Documenting tables
# MAGIC
# MAGIC In this example we are using the Catalog Explorer to view the `main.imdb.movies` table that we just created.  By clicking the 'AI generate' button we get a concise summary of the table description.  We then edit the description, adding a link to the original source of the dataset.
# MAGIC <br><br>
# MAGIC <img src="https://github.com/RafiKurlansik/notebook2/blob/main/assets/ai_generated_metadata.gif?raw=true" width=1000>

# COMMAND ----------

# MAGIC %md
# MAGIC ### Annotating columns
# MAGIC
# MAGIC Columns can also be auto-documented with generative AI. Being probabilistic in nature, sometimes AI gets things wrong - for example, it thinks that the `Stars` column refers to the number of stars given to it by critics.  In reality it refers to the starring cast for the film.  It's important to validate suggested comments before accepting them!<br><br>
# MAGIC
# MAGIC <img src="https://github.com/RafiKurlansik/notebook2/blob/main/assets/ai_generate_column_comments.gif?raw=true" width=700>

# COMMAND ----------

# MAGIC %md
# MAGIC ## Semantic search
# MAGIC
# MAGIC In contrast to keyword matching, generative AI makes it possible to search for data based on the subject or meaning of tables and columns in the Lakehouse.  You can see this in action by using the search bar at the top of the Workspace: <br> <br>
# MAGIC
# MAGIC <img src="https://github.com/RafiKurlansik/notebook2/blob/main/assets/semantic_search.gif?raw=true" width=800>
# MAGIC
# MAGIC What's remarkable about this is our search used colloquial phrases that don't necessarily match keywords precisely.  
# MAGIC
# MAGIC Furthermore, by annotating our table we were able to provide richer context to DatabricksIQ and improve the results of our search!
# MAGIC
